 \hypertarget{md__r_e_a_d_m_e_autotoc_md0}{}\doxysection{Experimental Final Assignment}\label{md__r_e_a_d_m_e_autotoc_md0}
\hypertarget{md__r_e_a_d_m_e_autotoc_md1}{}\doxysubsubsection{Table of Contents}\label{md__r_e_a_d_m_e_autotoc_md1}

\begin{DoxyItemize}
\item \href{\#experimental-final-assignment}{\texttt{ Experimental Final Assignment}}
\begin{DoxyItemize}
\item \href{\#table-of-contents}{\texttt{ Table of Contents}}
\end{DoxyItemize}
\end{DoxyItemize}

\href{\#introduction}{\texttt{ Introduction}}
\begin{DoxyItemize}
\item \href{\#software-architecture}{\texttt{ Software Architecture}}
\begin{DoxyItemize}
\item \href{\#nodes}{\texttt{ Nodes}}
\begin{DoxyItemize}
\item \href{\#command-manager-cmd_manpy}{\texttt{ Command Manager (cmd\+\_\+man.\+py)}}
\begin{DoxyItemize}
\item \href{\#state-normal}{\texttt{ State Normal}}
\item \href{\#state-sleep}{\texttt{ State Sleep}}
\item \href{\#state-play}{\texttt{ State Play}}
\item \href{\#state-find}{\texttt{ State Find}}
\end{DoxyItemize}
\item \href{\#opencv-balldetectionpy}{\texttt{ Open\+CV (Ball\+Detection.\+py)}}
\item \href{\#follow-wall-service-follow_wallpy}{\texttt{ Follow Wall Service (follow\+\_\+wall.\+py)}}
\item \href{\#user-interface-userplaypy}{\texttt{ User Interface (user\+Play.\+py)}}
\end{DoxyItemize}
\end{DoxyItemize}
\item \href{\#package-and-file-list}{\texttt{ Package and File List}}
\item \href{\#installation-and-running-procedure}{\texttt{ Installation and Running Procedure}}
\item \href{\#system-features}{\texttt{ System Features}}
\item \href{\#system-limitation-and-possible-solutions}{\texttt{ System Limitation and Possible Solutions}}
\item \href{\#author--contact}{\texttt{ Author \& Contact}}
\end{DoxyItemize}\hypertarget{md__r_e_a_d_m_e_autotoc_md2}{}\doxysubsection{Introduction}\label{md__r_e_a_d_m_e_autotoc_md2}
 {\itshape Fig. 1\+: Enviroment Top View}

This project is the further work of the previous \href{https://github.com/Sabrin0/Assignment2-Experimental-RoboticS-LAB.git}{\texttt{ assignment}} of {\itshape Experimental robotics lab}. Due to the \href{https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping}{\texttt{ SLAM}} and the \href{http://wiki.ros.org/navigation}{\texttt{ ROS Navighation Stack}} the wheeled robot is able to move autonomously. The main purpose of this project is to implement a FSM which allows the robot to learn the environment and interact with the user in order to reach the desired room. Each colored ball represents a different room (as shown below) and at the beginning of the simulation all the locations are unknown. It’s up to the robot to navigate and store the informations about the environment though a simple exploring algorithm.\hypertarget{md__r_e_a_d_m_e_autotoc_md3}{}\doxysubsection{Software Architecture}\label{md__r_e_a_d_m_e_autotoc_md3}
\hypertarget{md__r_e_a_d_m_e_autotoc_md4}{}\doxysubsubsection{Description}\label{md__r_e_a_d_m_e_autotoc_md4}
~ {\itshape Fig.\+2\+: Rosgraph}

As mentioned before, the current software architecture takes up the previous one. The main node is still the Command Manger, which implements a FSM. Basically the wheeled robot, equipped with an Hokuyo range finders and a RGB camera, follows four different behaviors\+:


\begin{DoxyItemize}
\item {\bfseries{Normal}}\+: in which the robot moves randomly in the environment
\item {\bfseries{Sleep}}\+: in which the robot comes back to the user position
\item {\bfseries{Play}}\+: in which the robot interacts with the user in order to receive command and reach specific rooms
\item {\bfseries{Find}}\+: in which the robot search the unkown rooms and stores their location
\end{DoxyItemize}

The navigation, including the global and the local path planning and the obstacles avoidance is provided by the package \href{http://wiki.ros.org/navigation}{\texttt{ ROS Navighation Stack}} while the \href{http://wiki.ros.org/gmapping}{\texttt{ gmapping}} supplies a 2-\/D occupancy grid map from laser and pose data collected by a mobile robot. In any case, the goal is set by the Command Manager depending on the current situation.\hypertarget{md__r_e_a_d_m_e_autotoc_md5}{}\doxysubsubsection{Nodes}\label{md__r_e_a_d_m_e_autotoc_md5}
\hypertarget{md__r_e_a_d_m_e_autotoc_md6}{}\doxyparagraph{Command Manager ($<$a href=\char`\"{}https\+://github.\+com/\+Sabrin0/exp\+\_\+final/blob/main/scripts/cmd\+\_\+man.\+py\char`\"{}$>$cmd\+\_\+man.\+py$<$/a$>$)}\label{md__r_e_a_d_m_e_autotoc_md6}
 {\itshape Fig.\+3\+: FST Diagram}

\label{md__r_e_a_d_m_e_autotoc_md7}%
\Hypertarget{md__r_e_a_d_m_e_autotoc_md7}%
 \doxysubparagraph*{State Normal}

In this state the robot moves randomly in the environment by sending several positions as goal to the Navigation Server. Here, whenever a ball is detected the robot start tracking it without cancelling the current goal. This is possible due to the fact that the open cv node controls directly the actuators by publishing on the topic {\itshape cmd\+\_\+vel}. Once the robot is close enough to the ball, it saves the current position of the ball by subscribing to the topic {\itshape odom}. After some iteration, the robot goes in the state {\bfseries{Normal}}. The command manager also subscribes to the topic {\itshape user\+Command}, so whenever it receives the command {\ttfamily play} it cancels the current goal and goes into the state {\bfseries{Play}}.

\label{md__r_e_a_d_m_e_autotoc_md8}%
\Hypertarget{md__r_e_a_d_m_e_autotoc_md8}%
 \doxysubparagraph*{State Sleep}

In this state the robot simply comes back to the user position located at the coordinates x = -\/8 and y = 8. Once arrived, after a while it returns the state {\bfseries{Normal}}.

\label{md__r_e_a_d_m_e_autotoc_md9}%
\Hypertarget{md__r_e_a_d_m_e_autotoc_md9}%
 \doxysubparagraph*{State Play}

Once the command manager receives the input command {\ttfamily play} from the {\bfseries{User Interface}}, the robot comes back to the home position and waits for a {\itshape Go\+To} command. If the location is known the robot will reach it, otherwise it goes to the state {\bfseries{Find}}. Moreover, the robots waits for the instructions 30 seconds then if no command is specified the robot goes back to the state {\bfseries{Normal}}.

\label{md__r_e_a_d_m_e_autotoc_md10}%
\Hypertarget{md__r_e_a_d_m_e_autotoc_md10}%
 \doxysubparagraph*{State Find}

This state is called from the {\bfseries{Play}} one, only if the location proposed by the user is unknown. It acts as a {\itshape client} for the \href{https://github.com/Sabrin0/exp_final/blob/main/scripts/follow_wall.py}{\texttt{ follow\+\_\+wall}} service by which the robot starts to explore the environment, following the walls. The main purpose of this exploration is to find the desired room and store the information about its coordinates. After 5 minutes, if the required room has not be found, the robot goes back to the state {\bfseries{Play}}. In any case, the other unknown rooms are tracked (if found). Given the fact that the robot always starts exploring from the user position and whereas the robot follows the wall in a {\itshape anti-\/clockwise direction} some room would ever find (for instance the bedroom). For this reason, a simple {\itshape pre-\/find} algorithm has been implemented in order to go first at the last known room visited ({\itshape always from an anti-\/clockwise direction pov}). In this phase, the algorithm iterates among the room indexes ({\itshape from blue to black}) and set as navigation goal the location that precedes the first unknown one found.\hypertarget{md__r_e_a_d_m_e_autotoc_md11}{}\doxyparagraph{Open\+CV ($<$a href=\char`\"{}https\+://github.\+com/\+Sabrin0/exp\+\_\+final/blob/main/scripts/\+Ball\+Detection.\+py\char`\"{}$>$\+Ball\+Detection.\+py$<$/a$>$)}\label{md__r_e_a_d_m_e_autotoc_md11}
This node manages the ball detection and therefore the tracking by processing the image received from the robot RGB camera. It\textquotesingle{}s characterized by two main phases\+:


\begin{DoxyItemize}
\item {\bfseries{\href{https://github.com/Sabrin0/exp_final/blob/main/scripts/colorLabeler.py}{\texttt{ Pre-\/processing}}}} \+: in which an algorithm performs object and color detection, in order to identify any ball in the environment and recognizes its color. Then it returns the upper and lower masks required in the next phase.
\item {\bfseries{Tracking}}\+: Depending on several conditions, for instances the current state of the robot (updated by the topic {\itshape current\+State}) and if one specific ball has not been already detected, the algorithm in this phase starts tracking the ball.
\end{DoxyItemize}

This node subscribes to two main topics\+:


\begin{DoxyItemize}
\item {\itshape Ball\+State}\+: By which the command manager is up to date regarding the position of the robot wrt the tracked ball. Once the robot is sufficiently near to the ball, in the command manager the specific location is stored.
\item {\itshape cmd\+\_\+vel}\+: By which the robot movement is controlled in order to reach the ball. Publishing directly on the aforementioned topic, the Navigation Stack {\itshape loses its priority}.
\end{DoxyItemize}\hypertarget{md__r_e_a_d_m_e_autotoc_md12}{}\doxyparagraph{Follow Wall Service ($<$a href=\char`\"{}https\+://github.\+com/\+Sabrin0/exp\+\_\+final/blob/main/scripts/follow\+\_\+wall.\+py\char`\"{}$>$follow\+\_\+wall.\+py$<$/a$>$)}\label{md__r_e_a_d_m_e_autotoc_md12}
 {\itshape Fig.\+5\+: Laser fov subdivided into 5 regions}

This node is a ROS server that provides the exploration of the environment and it is activated by the command manager, which acts as a Client, inside the state {\bfseries{Find}}. It relies on the laser data provided by the topic {\itshape Scan}. In particular the {\itshape fov} of the sensor is divided into 5 regions, then depending on which of them detect an obstacle (and at what distance) the exploration switches among three different navigation state\+:


\begin{DoxyItemize}
\item {\bfseries{find wall}}\+: the robot looks for a near wall with a circular motion
\item {\bfseries{follow wall}}\+: the robot follows the wall detected keeping a certain distance
\item {\bfseries{turn left}}\+: the robot simply turns left (only angular velocity among z) in order to avoid getting stuck.
\end{DoxyItemize}

The navigation is ensured by publishing on the topic {\itshape cmd\+\_\+vel}.\hypertarget{md__r_e_a_d_m_e_autotoc_md13}{}\doxyparagraph{User Interface ($<$a href=\char`\"{}https\+://github.\+com/\+Sabrin0/exp\+\_\+final/blob/main/scripts/user\+Play.\+py\char`\"{}$>$user\+Play.\+py$<$/a$>$)}\label{md__r_e_a_d_m_e_autotoc_md13}

\begin{DoxyCode}{0}
\DoxyCodeLine{user@hostname\$          |\(\backslash\)\_/|                  }
\DoxyCodeLine{                        | @ @   Woof! }
\DoxyCodeLine{                        |   <>              \_  }
\DoxyCodeLine{                        |  \_/\(\backslash\)-\/-\/-\/-\/-\/-\/\_\_\_\_ ((| |))}
\DoxyCodeLine{                        |               `-\/-\/' |   }
\DoxyCodeLine{                    \_\_\_\_|\_       \_\_\_|   |\_\_\_.' }
\DoxyCodeLine{                /\_/\_\_\_\_\_/\_\_\_\_/\_\_\_\_\_\_\_|}
\DoxyCodeLine{                            Welcome! }
\DoxyCodeLine{}
\DoxyCodeLine{Please enter 'play' to call the dog:}

\end{DoxyCode}
 {\itshape Output of the UI}

This node provides the communication between the user and the command manager though the shell by publishing on the topic {\itshape user\+State} and moreover it is available only when the robot is in the state {\bfseries{Normal}}. First of all, the user can call back the robot by entering the string {\ttfamily play}. Once the robot has reached the user position, it’s possible to enter a new command, in details the color related to a specific room, in order to make the let the robot move to the desired location.\hypertarget{md__r_e_a_d_m_e_autotoc_md14}{}\doxysubsection{Package and File List}\label{md__r_e_a_d_m_e_autotoc_md14}

\begin{DoxyCode}{0}
\DoxyCodeLine{├── CMakeLists.txt}
\DoxyCodeLine{├── Doxyfile}
\DoxyCodeLine{├── README.md}
\DoxyCodeLine{├── assets}
\DoxyCodeLine{├── config}
\DoxyCodeLine{│   └── sim2.rviz}
\DoxyCodeLine{├── docs}
\DoxyCodeLine{├── launch}
\DoxyCodeLine{│   ├── gmapping.launch}
\DoxyCodeLine{│   ├── move\_base.launch}
\DoxyCodeLine{│   └── simulation.launch}
\DoxyCodeLine{├── msg}
\DoxyCodeLine{│   ├── BallState.msg}
\DoxyCodeLine{│   └── user.msg}
\DoxyCodeLine{├── package.xml}
\DoxyCodeLine{├── param}
\DoxyCodeLine{│   ├── base\_local\_planner\_params.yaml}
\DoxyCodeLine{│   ├── costmap\_common\_params.yaml}
\DoxyCodeLine{│   ├── global\_costmap\_params.yaml}
\DoxyCodeLine{│   ├── local\_costmap\_params.yaml}
\DoxyCodeLine{│   └── move\_base\_params.yaml}
\DoxyCodeLine{├── scripts}
\DoxyCodeLine{│   ├── BallDetection.py}
\DoxyCodeLine{│   ├── cmd\_man.py}
\DoxyCodeLine{│   ├── colorLabeler.py}
\DoxyCodeLine{│   ├── follow\_wall.py}
\DoxyCodeLine{│   └── userPlay.py}
\DoxyCodeLine{├── urdf}
\DoxyCodeLine{│   ├── human.urdf}
\DoxyCodeLine{│   ├── robot2\_laser.gazebo}
\DoxyCodeLine{│   ├── robot2\_laser.urdf}
\DoxyCodeLine{│   └── robot2\_laser.xacro}
\DoxyCodeLine{└── worlds}
\DoxyCodeLine{    └── house2.world}

\end{DoxyCode}
 {\itshape exp\+\_\+final directory Tree}

TThe main files are stored inside the following directories\+:


\begin{DoxyItemize}
\item {\bfseries{scripts}}\+: where all the nodes are stored
\item {\bfseries{msg}}\+: where the ROS custom msgs are defined. In particular the the following ones\+:
\begin{DoxyItemize}
\item {\itshape Ball\+State.\+msg}\+: \`{}\`{}\`{} bool Ball\+Detected float64 currrent\+Radious string bal\+Color \`{}\`{}\`{} Here all the messages related to the ball info are shared via topic {\itshape Ball\+State}.
\item {\itshape user.\+msg}\+: \`{}\`{}\`{} bool play string color \`{}\`{}\`{} Here all the messages related to the user command are shared via topic {\itshape user\+Command}.
\end{DoxyItemize}
\item {\bfseries{launch}}\+: ROS launch files for starting the simulation
\item {\bfseries{param}}\+: files .yaml used to configure the gmapping and the Navigation Stack
\item {\bfseries{urdf}}\+: file with the description of the element spawned in the enviroment
\end{DoxyItemize}

The other directories contain file for build the ROS workspaces and the documentation.\hypertarget{md__r_e_a_d_m_e_autotoc_md15}{}\doxysubsection{Installation and Running Procedure}\label{md__r_e_a_d_m_e_autotoc_md15}
As mentioned in the introduction this architecture is based on the ROS Navigation Stack and the gmapping, so please install\+: For the openslam\+\_\+gmapping\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{user@hostanme\$ sudo apt-\/get install ros-\/<ros\_distro>-\/openslam-\/gmapping}

\end{DoxyCode}
 For the sparse library\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{user@hostanme\$ sudo apt-\/get install libsuitesparse-\/dev}

\end{DoxyCode}
 For the ROS Navigation Stack\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{user@hostanme\$ sudo apt-\/get install ros-\/<ros\_distro>-\/navigation}

\end{DoxyCode}


Then in order to configure the ROS environment inside the current workspace the commands {\ttfamily catkin\+\_\+make} is required as well as the sourcing of the setup.$\ast$sh files. So, finally, to start the simulation please run\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{user@hostname\$ roslaunch exp\_final simulation.launch}

\end{DoxyCode}


You’ll notice that two new shell windows will appear after the execution of the launch. One is the output of the node {\itshape \mbox{\hyperlink{cmd__man_8py}{cmd\+\_\+man.\+py}}} and the other one is the {\itshape user interface}.\hypertarget{md__r_e_a_d_m_e_autotoc_md16}{}\doxysubsection{System Features}\label{md__r_e_a_d_m_e_autotoc_md16}
 {\itshape Fig.\+5\+: Robot Model}

The model of the enviroment is a simple wheeled robot (two actuated wheels and a castor one) equipped with an Hokuyo range finders and a RGB camera. The model itself is described in the xacro file \href{https://github.com/Sabrin0/exp_final/blob/main/urdf/robot2_laser.xacro}{\texttt{ robot2\+\_\+laser.\+xacro}} while in the \href{https://github.com/Sabrin0/exp_final/blob/main/urdf/robot2_laser.gazebo}{\texttt{ robot2\+\_\+laser.\+gazebo}} all the parameters related to the configuration of the sensor are defined.\hypertarget{md__r_e_a_d_m_e_autotoc_md17}{}\doxysubsection{System Limitation and Possible Solutions}\label{md__r_e_a_d_m_e_autotoc_md17}
One of the main limitation of the system is the mobility during the state find. I decided to recall and improve the navigation proposed by the bug algorithm despite of using the \href{https://github.com/Sabrin0/exp_final/blob/main/urdf/robot2_laser.xacro}{\texttt{ explore-\/lite pakage}}. Unlike this last one the current implementation is not based on a frontier-\/based exploration but instead the robot simply follows the wall. For this reason the motion is very slow and there is the high-\/risk that the robot gets stuck when it try to go though a narrow door. Aside from that, in this way there is the certain that the robot will found all the ball in the environment. As said before, a possible solution is implement a frontier-\/based exploration with the obstacle avoidance asset. Another problem is related to the open\+CV algorithm. Since it detects all the circles in the processed image there is the possibility that there is a false-\/positive. This problem can be resolved with an update of the dictionary related to the color detection, by enriching it whit more colors. In this way the robot will know in advance which ball track and which do not. Moreover during the track phase, the obstacle avoidance is not very effective since the open\+CV algorithm directly control the topic {\itshape cmd\+\_\+vel} (responsible of the wheels actuation).\hypertarget{md__r_e_a_d_m_e_autotoc_md18}{}\doxysubsection{Author \& Contact}\label{md__r_e_a_d_m_e_autotoc_md18}
Luca Covizzi

\href{mailto:luca@francocovizzi.it}{\texttt{ luca@francocovizzi.\+it}} 